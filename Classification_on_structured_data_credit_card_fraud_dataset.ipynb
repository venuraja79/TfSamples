{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification on structured data: credit card fraud dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venuraja79/TfSamples/blob/master/Classification_on_structured_data_credit_card_fraud_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTPmTZKc6770",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRHmSyHxEIhN",
        "colab_type": "text"
      },
      "source": [
        "## First, vectorize the CSV data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG5iYmD-7dZh",
        "colab_type": "code",
        "outputId": "bea79c47-bed6-44ce-f6e6-561c286518d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/downloads/creditcardfraud.zip/\n",
        "fname = '/Users/fchollet/Downloads/creditcard.csv'\n",
        "\n",
        "all_features = []\n",
        "all_targets = []\n",
        "with open(fname) as f:\n",
        "  for i, line in enumerate(f):\n",
        "    if i == 0:\n",
        "      print('HEADER:', line.strip())\n",
        "      continue  # Skip header\n",
        "    fields = line.strip().split(',')\n",
        "    all_features.append([float(v.replace('\"', '')) for v in fields[:-1]])\n",
        "    all_targets.append([int(fields[-1].replace('\"', ''))])\n",
        "    if i == 1:\n",
        "      print('EXAMPLE FEATURES:', all_features[-1])\n",
        "    \n",
        "features = np.array(all_features, dtype='float32')\n",
        "targets = np.array(all_targets, dtype='uint8')\n",
        "print('features.shape:', features.shape)\n",
        "print('targets.shape:', targets.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEADER: \"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
            "EXAMPLE FEATURES: [0.0, -1.3598071336738, -0.0727811733098497, 2.53634673796914, 1.37815522427443, -0.338320769942518, 0.462387777762292, 0.239598554061257, 0.0986979012610507, 0.363786969611213, 0.0907941719789316, -0.551599533260813, -0.617800855762348, -0.991389847235408, -0.311169353699879, 1.46817697209427, -0.470400525259478, 0.207971241929242, 0.0257905801985591, 0.403992960255733, 0.251412098239705, -0.018306777944153, 0.277837575558899, -0.110473910188767, 0.0669280749146731, 0.128539358273528, -0.189114843888824, 0.133558376740387, -0.0210530534538215, 149.62]\n",
            "features.shape: (284807, 30)\n",
            "targets.shape: (284807, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymKKJ7MLEMJZ",
        "colab_type": "text"
      },
      "source": [
        "## Prepare a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FDeMVhH7tLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_val_samples = int(len(features) * 0.2)\n",
        "train_features = features[:-num_val_samples]\n",
        "train_targets = targets[:-num_val_samples]\n",
        "val_features = features[-num_val_samples:]\n",
        "val_targets = targets[-num_val_samples:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYMuiKYzInMy",
        "colab_type": "code",
        "outputId": "ec68e2c9-afe6-42db-8cdc-bdc44c181768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Number of training samples:', len(train_features))\n",
        "print('Number of validation samples:', len(val_features))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 227846\n",
            "Number of validation samples: 56961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWKB_CVZFLpB",
        "colab_type": "text"
      },
      "source": [
        "## Analyze class imbalance in the targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKWfTCcyFN2K",
        "colab_type": "code",
        "outputId": "212eb61e-e87a-4f75-babf-8dd996ef6051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "counts = np.bincount(train_targets[:, 0])\n",
        "print('Number of positive samples in training data: {} ({:.2f}% of total)'.format(counts[1], 100 * float(counts[1]) / len(train_targets)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive samples in training data: 417 (0.18% of total)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjGWErngGny7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_for_0 = 1. / counts[0]\n",
        "weight_for_1 = 1. / counts[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNa3j61UEOd7",
        "colab_type": "text"
      },
      "source": [
        "## Normalize the data using training set statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkdwBiriD4Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = np.mean(train_features, axis=0)\n",
        "train_features -= mean\n",
        "val_features -= mean\n",
        "std = np.std(train_features, axis=0)\n",
        "train_features /= std\n",
        "val_features /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIMJ6ebpD6qG",
        "colab_type": "code",
        "outputId": "b72a7709-6cd1-4e92-bec1-d05311863618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "  keras.layers.Dense(256, activation='relu',\n",
        "                     input_shape=(train_features.shape[-1],)),\n",
        "  keras.layers.Dense(256, activation='relu'),\n",
        "  keras.layers.Dropout(0.3),\n",
        "  keras.layers.Dense(256, activation='relu'),\n",
        "  keras.layers.Dropout(0.3),\n",
        "  keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 256)               7936      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 139,777\n",
            "Trainable params: 139,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REP1KlrSEx-m",
        "colab_type": "code",
        "outputId": "58f3f6c3-25f7-4a9c-d2a1-ea358522fa3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1771
        }
      },
      "source": [
        "metrics = [keras.metrics.FalseNegatives(name='fn'),\n",
        "           keras.metrics.FalsePositives(name='fp'),\n",
        "           keras.metrics.TrueNegatives(name='tn'),\n",
        "           keras.metrics.TruePositives(name='tp'),\n",
        "           keras.metrics.Precision(name='precision'),\n",
        "           keras.metrics.Recall(name='recall')]\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-2),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=metrics)\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint('fraud_model_at_epoch_{epoch}.h5')]\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "model.fit(train_features, train_targets,\n",
        "          batch_size=2048,\n",
        "          epochs=50,\n",
        "          verbose=2,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(val_features, val_targets),\n",
        "          class_weight=class_weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 227846 samples, validate on 56961 samples\n",
            "Epoch 1/50\n",
            "227846/227846 - 2s - loss: 2.4066e-06 - fn: 45.0000 - fp: 24340.0000 - tn: 203089.0000 - tp: 372.0000 - precision: 0.0151 - recall: 0.8921 - val_loss: 0.1311 - val_fn: 10.0000 - val_fp: 1103.0000 - val_tn: 55783.0000 - val_tp: 65.0000 - val_precision: 0.0557 - val_recall: 0.8667\n",
            "Epoch 2/50\n",
            "227846/227846 - 1s - loss: 1.4066e-06 - fn: 29.0000 - fp: 8700.0000 - tn: 218729.0000 - tp: 388.0000 - precision: 0.0427 - recall: 0.9305 - val_loss: 0.1044 - val_fn: 7.0000 - val_fp: 1521.0000 - val_tn: 55365.0000 - val_tp: 68.0000 - val_precision: 0.0428 - val_recall: 0.9067\n",
            "Epoch 3/50\n",
            "227846/227846 - 1s - loss: 1.1769e-06 - fn: 30.0000 - fp: 6944.0000 - tn: 220485.0000 - tp: 387.0000 - precision: 0.0528 - recall: 0.9281 - val_loss: 0.1048 - val_fn: 8.0000 - val_fp: 1291.0000 - val_tn: 55595.0000 - val_tp: 67.0000 - val_precision: 0.0493 - val_recall: 0.8933\n",
            "Epoch 4/50\n",
            "227846/227846 - 1s - loss: 1.1076e-06 - fn: 23.0000 - fp: 7688.0000 - tn: 219741.0000 - tp: 394.0000 - precision: 0.0488 - recall: 0.9448 - val_loss: 0.0503 - val_fn: 11.0000 - val_fp: 446.0000 - val_tn: 56440.0000 - val_tp: 64.0000 - val_precision: 0.1255 - val_recall: 0.8533\n",
            "Epoch 5/50\n",
            "227846/227846 - 1s - loss: 9.0560e-07 - fn: 22.0000 - fp: 6160.0000 - tn: 221269.0000 - tp: 395.0000 - precision: 0.0603 - recall: 0.9472 - val_loss: 0.0676 - val_fn: 6.0000 - val_fp: 1284.0000 - val_tn: 55602.0000 - val_tp: 69.0000 - val_precision: 0.0510 - val_recall: 0.9200\n",
            "Epoch 6/50\n",
            "227846/227846 - 1s - loss: 9.1067e-07 - fn: 17.0000 - fp: 7555.0000 - tn: 219874.0000 - tp: 400.0000 - precision: 0.0503 - recall: 0.9592 - val_loss: 0.0411 - val_fn: 10.0000 - val_fp: 477.0000 - val_tn: 56409.0000 - val_tp: 65.0000 - val_precision: 0.1199 - val_recall: 0.8667\n",
            "Epoch 7/50\n",
            "227846/227846 - 1s - loss: 6.6823e-07 - fn: 15.0000 - fp: 5903.0000 - tn: 221526.0000 - tp: 402.0000 - precision: 0.0638 - recall: 0.9640 - val_loss: 0.0289 - val_fn: 11.0000 - val_fp: 473.0000 - val_tn: 56413.0000 - val_tp: 64.0000 - val_precision: 0.1192 - val_recall: 0.8533\n",
            "Epoch 8/50\n",
            "227846/227846 - 1s - loss: 7.9562e-07 - fn: 16.0000 - fp: 7282.0000 - tn: 220147.0000 - tp: 401.0000 - precision: 0.0522 - recall: 0.9616 - val_loss: 0.0866 - val_fn: 7.0000 - val_fp: 1951.0000 - val_tn: 54935.0000 - val_tp: 68.0000 - val_precision: 0.0337 - val_recall: 0.9067\n",
            "Epoch 9/50\n",
            "227846/227846 - 1s - loss: 7.0538e-07 - fn: 13.0000 - fp: 7233.0000 - tn: 220196.0000 - tp: 404.0000 - precision: 0.0529 - recall: 0.9688 - val_loss: 0.0519 - val_fn: 9.0000 - val_fp: 939.0000 - val_tn: 55947.0000 - val_tp: 66.0000 - val_precision: 0.0657 - val_recall: 0.8800\n",
            "Epoch 10/50\n",
            "227846/227846 - 1s - loss: 6.4500e-07 - fn: 15.0000 - fp: 5864.0000 - tn: 221565.0000 - tp: 402.0000 - precision: 0.0642 - recall: 0.9640 - val_loss: 0.1395 - val_fn: 6.0000 - val_fp: 3020.0000 - val_tn: 53866.0000 - val_tp: 69.0000 - val_precision: 0.0223 - val_recall: 0.9200\n",
            "Epoch 11/50\n",
            "227846/227846 - 1s - loss: 5.3056e-07 - fn: 10.0000 - fp: 5568.0000 - tn: 221861.0000 - tp: 407.0000 - precision: 0.0681 - recall: 0.9760 - val_loss: 0.0406 - val_fn: 10.0000 - val_fp: 988.0000 - val_tn: 55898.0000 - val_tp: 65.0000 - val_precision: 0.0617 - val_recall: 0.8667\n",
            "Epoch 12/50\n",
            "227846/227846 - 1s - loss: 5.6445e-07 - fn: 10.0000 - fp: 6535.0000 - tn: 220894.0000 - tp: 407.0000 - precision: 0.0586 - recall: 0.9760 - val_loss: 0.0194 - val_fn: 8.0000 - val_fp: 428.0000 - val_tn: 56458.0000 - val_tp: 67.0000 - val_precision: 0.1354 - val_recall: 0.8933\n",
            "Epoch 13/50\n",
            "227846/227846 - 1s - loss: 4.0544e-07 - fn: 6.0000 - fp: 4004.0000 - tn: 223425.0000 - tp: 411.0000 - precision: 0.0931 - recall: 0.9856 - val_loss: 0.0511 - val_fn: 8.0000 - val_fp: 1083.0000 - val_tn: 55803.0000 - val_tp: 67.0000 - val_precision: 0.0583 - val_recall: 0.8933\n",
            "Epoch 14/50\n",
            "227846/227846 - 1s - loss: 5.0580e-07 - fn: 6.0000 - fp: 5623.0000 - tn: 221806.0000 - tp: 411.0000 - precision: 0.0681 - recall: 0.9856 - val_loss: 0.2466 - val_fn: 10.0000 - val_fp: 3120.0000 - val_tn: 53766.0000 - val_tp: 65.0000 - val_precision: 0.0204 - val_recall: 0.8667\n",
            "Epoch 15/50\n",
            "227846/227846 - 1s - loss: 1.3510e-06 - fn: 20.0000 - fp: 8987.0000 - tn: 218442.0000 - tp: 397.0000 - precision: 0.0423 - recall: 0.9520 - val_loss: 0.0354 - val_fn: 11.0000 - val_fp: 595.0000 - val_tn: 56291.0000 - val_tp: 64.0000 - val_precision: 0.0971 - val_recall: 0.8533\n",
            "Epoch 16/50\n",
            "227846/227846 - 1s - loss: 8.4579e-07 - fn: 11.0000 - fp: 7312.0000 - tn: 220117.0000 - tp: 406.0000 - precision: 0.0526 - recall: 0.9736 - val_loss: 0.0569 - val_fn: 9.0000 - val_fp: 888.0000 - val_tn: 55998.0000 - val_tp: 66.0000 - val_precision: 0.0692 - val_recall: 0.8800\n",
            "Epoch 17/50\n",
            "227846/227846 - 1s - loss: 5.7654e-07 - fn: 12.0000 - fp: 5131.0000 - tn: 222298.0000 - tp: 405.0000 - precision: 0.0732 - recall: 0.9712 - val_loss: 0.0395 - val_fn: 10.0000 - val_fp: 703.0000 - val_tn: 56183.0000 - val_tp: 65.0000 - val_precision: 0.0846 - val_recall: 0.8667\n",
            "Epoch 18/50\n",
            "227846/227846 - 1s - loss: 4.0902e-07 - fn: 7.0000 - fp: 4468.0000 - tn: 222961.0000 - tp: 410.0000 - precision: 0.0841 - recall: 0.9832 - val_loss: 0.0256 - val_fn: 9.0000 - val_fp: 497.0000 - val_tn: 56389.0000 - val_tp: 66.0000 - val_precision: 0.1172 - val_recall: 0.8800\n",
            "Epoch 19/50\n",
            "227846/227846 - 1s - loss: 4.6018e-07 - fn: 5.0000 - fp: 3459.0000 - tn: 223970.0000 - tp: 412.0000 - precision: 0.1064 - recall: 0.9880 - val_loss: 0.0183 - val_fn: 10.0000 - val_fp: 347.0000 - val_tn: 56539.0000 - val_tp: 65.0000 - val_precision: 0.1578 - val_recall: 0.8667\n",
            "Epoch 20/50\n",
            "227846/227846 - 1s - loss: 4.0104e-07 - fn: 4.0000 - fp: 2684.0000 - tn: 224745.0000 - tp: 413.0000 - precision: 0.1334 - recall: 0.9904 - val_loss: 0.0183 - val_fn: 10.0000 - val_fp: 285.0000 - val_tn: 56601.0000 - val_tp: 65.0000 - val_precision: 0.1857 - val_recall: 0.8667\n",
            "Epoch 21/50\n",
            "227846/227846 - 1s - loss: 7.1501e-07 - fn: 6.0000 - fp: 5366.0000 - tn: 222063.0000 - tp: 411.0000 - precision: 0.0711 - recall: 0.9856 - val_loss: 0.0463 - val_fn: 8.0000 - val_fp: 560.0000 - val_tn: 56326.0000 - val_tp: 67.0000 - val_precision: 0.1069 - val_recall: 0.8933\n",
            "Epoch 22/50\n",
            "227846/227846 - 1s - loss: 5.4339e-07 - fn: 5.0000 - fp: 5178.0000 - tn: 222251.0000 - tp: 412.0000 - precision: 0.0737 - recall: 0.9880 - val_loss: 0.0310 - val_fn: 9.0000 - val_fp: 455.0000 - val_tn: 56431.0000 - val_tp: 66.0000 - val_precision: 0.1267 - val_recall: 0.8800\n",
            "Epoch 23/50\n",
            "227846/227846 - 1s - loss: 3.7788e-07 - fn: 4.0000 - fp: 3559.0000 - tn: 223870.0000 - tp: 413.0000 - precision: 0.1040 - recall: 0.9904 - val_loss: 0.0198 - val_fn: 13.0000 - val_fp: 258.0000 - val_tn: 56628.0000 - val_tp: 62.0000 - val_precision: 0.1937 - val_recall: 0.8267\n",
            "Epoch 24/50\n",
            "227846/227846 - 1s - loss: 3.2270e-07 - fn: 1.0000 - fp: 3582.0000 - tn: 223847.0000 - tp: 416.0000 - precision: 0.1041 - recall: 0.9976 - val_loss: 0.0178 - val_fn: 13.0000 - val_fp: 378.0000 - val_tn: 56508.0000 - val_tp: 62.0000 - val_precision: 0.1409 - val_recall: 0.8267\n",
            "Epoch 25/50\n",
            "227846/227846 - 1s - loss: 4.5065e-07 - fn: 4.0000 - fp: 3260.0000 - tn: 224169.0000 - tp: 413.0000 - precision: 0.1124 - recall: 0.9904 - val_loss: 0.0355 - val_fn: 9.0000 - val_fp: 582.0000 - val_tn: 56304.0000 - val_tp: 66.0000 - val_precision: 0.1019 - val_recall: 0.8800\n",
            "Epoch 26/50\n",
            "227846/227846 - 1s - loss: 2.5657e-07 - fn: 3.0000 - fp: 2680.0000 - tn: 224749.0000 - tp: 414.0000 - precision: 0.1338 - recall: 0.9928 - val_loss: 0.0157 - val_fn: 12.0000 - val_fp: 246.0000 - val_tn: 56640.0000 - val_tp: 63.0000 - val_precision: 0.2039 - val_recall: 0.8400\n",
            "Epoch 27/50\n",
            "227846/227846 - 1s - loss: 3.7241e-07 - fn: 3.0000 - fp: 2286.0000 - tn: 225143.0000 - tp: 414.0000 - precision: 0.1533 - recall: 0.9928 - val_loss: 0.0133 - val_fn: 11.0000 - val_fp: 239.0000 - val_tn: 56647.0000 - val_tp: 64.0000 - val_precision: 0.2112 - val_recall: 0.8533\n",
            "Epoch 28/50\n",
            "227846/227846 - 1s - loss: 9.5310e-07 - fn: 6.0000 - fp: 4061.0000 - tn: 223368.0000 - tp: 411.0000 - precision: 0.0919 - recall: 0.9856 - val_loss: 0.1881 - val_fn: 10.0000 - val_fp: 1799.0000 - val_tn: 55087.0000 - val_tp: 65.0000 - val_precision: 0.0349 - val_recall: 0.8667\n",
            "Epoch 29/50\n",
            "227846/227846 - 1s - loss: 2.7635e-06 - fn: 23.0000 - fp: 8688.0000 - tn: 218741.0000 - tp: 394.0000 - precision: 0.0434 - recall: 0.9448 - val_loss: 0.1453 - val_fn: 9.0000 - val_fp: 1678.0000 - val_tn: 55208.0000 - val_tp: 66.0000 - val_precision: 0.0378 - val_recall: 0.8800\n",
            "Epoch 30/50\n",
            "227846/227846 - 1s - loss: 1.9673e-06 - fn: 13.0000 - fp: 8870.0000 - tn: 218559.0000 - tp: 404.0000 - precision: 0.0436 - recall: 0.9688 - val_loss: 0.4516 - val_fn: 8.0000 - val_fp: 2179.0000 - val_tn: 54707.0000 - val_tp: 67.0000 - val_precision: 0.0298 - val_recall: 0.8933\n",
            "Epoch 31/50\n",
            "227846/227846 - 1s - loss: 1.6888e-06 - fn: 15.0000 - fp: 8113.0000 - tn: 219316.0000 - tp: 402.0000 - precision: 0.0472 - recall: 0.9640 - val_loss: 0.0751 - val_fn: 10.0000 - val_fp: 679.0000 - val_tn: 56207.0000 - val_tp: 65.0000 - val_precision: 0.0874 - val_recall: 0.8667\n",
            "Epoch 32/50\n",
            "227846/227846 - 1s - loss: 8.3537e-07 - fn: 9.0000 - fp: 5096.0000 - tn: 222333.0000 - tp: 408.0000 - precision: 0.0741 - recall: 0.9784 - val_loss: 0.0914 - val_fn: 9.0000 - val_fp: 717.0000 - val_tn: 56169.0000 - val_tp: 66.0000 - val_precision: 0.0843 - val_recall: 0.8800\n",
            "Epoch 33/50\n",
            "227846/227846 - 1s - loss: 4.1038e-07 - fn: 5.0000 - fp: 3061.0000 - tn: 224368.0000 - tp: 412.0000 - precision: 0.1186 - recall: 0.9880 - val_loss: 0.0262 - val_fn: 10.0000 - val_fp: 523.0000 - val_tn: 56363.0000 - val_tp: 65.0000 - val_precision: 0.1105 - val_recall: 0.8667\n",
            "Epoch 34/50\n",
            "227846/227846 - 1s - loss: 8.5043e-07 - fn: 6.0000 - fp: 4219.0000 - tn: 223210.0000 - tp: 411.0000 - precision: 0.0888 - recall: 0.9856 - val_loss: 0.1486 - val_fn: 10.0000 - val_fp: 743.0000 - val_tn: 56143.0000 - val_tp: 65.0000 - val_precision: 0.0804 - val_recall: 0.8667\n",
            "Epoch 35/50\n",
            "227846/227846 - 1s - loss: 8.4945e-07 - fn: 5.0000 - fp: 4857.0000 - tn: 222572.0000 - tp: 412.0000 - precision: 0.0782 - recall: 0.9880 - val_loss: 0.1016 - val_fn: 9.0000 - val_fp: 689.0000 - val_tn: 56197.0000 - val_tp: 66.0000 - val_precision: 0.0874 - val_recall: 0.8800\n",
            "Epoch 36/50\n",
            "227846/227846 - 1s - loss: 4.0998e-07 - fn: 5.0000 - fp: 3584.0000 - tn: 223845.0000 - tp: 412.0000 - precision: 0.1031 - recall: 0.9880 - val_loss: 0.0160 - val_fn: 10.0000 - val_fp: 209.0000 - val_tn: 56677.0000 - val_tp: 65.0000 - val_precision: 0.2372 - val_recall: 0.8667\n",
            "Epoch 37/50\n",
            "227846/227846 - 1s - loss: 4.7637e-07 - fn: 5.0000 - fp: 2964.0000 - tn: 224465.0000 - tp: 412.0000 - precision: 0.1220 - recall: 0.9880 - val_loss: 0.0344 - val_fn: 10.0000 - val_fp: 446.0000 - val_tn: 56440.0000 - val_tp: 65.0000 - val_precision: 0.1272 - val_recall: 0.8667\n",
            "Epoch 38/50\n",
            "227846/227846 - 1s - loss: 4.5030e-07 - fn: 4.0000 - fp: 3563.0000 - tn: 223866.0000 - tp: 413.0000 - precision: 0.1039 - recall: 0.9904 - val_loss: 0.0135 - val_fn: 11.0000 - val_fp: 193.0000 - val_tn: 56693.0000 - val_tp: 64.0000 - val_precision: 0.2490 - val_recall: 0.8533\n",
            "Epoch 39/50\n",
            "227846/227846 - 1s - loss: 4.1618e-07 - fn: 5.0000 - fp: 3152.0000 - tn: 224277.0000 - tp: 412.0000 - precision: 0.1156 - recall: 0.9880 - val_loss: 0.0915 - val_fn: 8.0000 - val_fp: 914.0000 - val_tn: 55972.0000 - val_tp: 67.0000 - val_precision: 0.0683 - val_recall: 0.8933\n",
            "Epoch 40/50\n",
            "227846/227846 - 1s - loss: 8.9481e-07 - fn: 3.0000 - fp: 4645.0000 - tn: 222784.0000 - tp: 414.0000 - precision: 0.0818 - recall: 0.9928 - val_loss: 0.1184 - val_fn: 13.0000 - val_fp: 628.0000 - val_tn: 56258.0000 - val_tp: 62.0000 - val_precision: 0.0899 - val_recall: 0.8267\n",
            "Epoch 41/50\n",
            "227846/227846 - 1s - loss: 9.0374e-07 - fn: 5.0000 - fp: 4340.0000 - tn: 223089.0000 - tp: 412.0000 - precision: 0.0867 - recall: 0.9880 - val_loss: 0.1500 - val_fn: 10.0000 - val_fp: 914.0000 - val_tn: 55972.0000 - val_tp: 65.0000 - val_precision: 0.0664 - val_recall: 0.8667\n",
            "Epoch 42/50\n",
            "227846/227846 - 1s - loss: 1.1963e-06 - fn: 8.0000 - fp: 5877.0000 - tn: 221552.0000 - tp: 409.0000 - precision: 0.0651 - recall: 0.9808 - val_loss: 0.6180 - val_fn: 7.0000 - val_fp: 3028.0000 - val_tn: 53858.0000 - val_tp: 68.0000 - val_precision: 0.0220 - val_recall: 0.9067\n",
            "Epoch 43/50\n",
            "227846/227846 - 1s - loss: 1.0751e-06 - fn: 6.0000 - fp: 5344.0000 - tn: 222085.0000 - tp: 411.0000 - precision: 0.0714 - recall: 0.9856 - val_loss: 0.0430 - val_fn: 8.0000 - val_fp: 374.0000 - val_tn: 56512.0000 - val_tp: 67.0000 - val_precision: 0.1519 - val_recall: 0.8933\n",
            "Epoch 44/50\n",
            "227846/227846 - 1s - loss: 5.0250e-07 - fn: 4.0000 - fp: 3650.0000 - tn: 223779.0000 - tp: 413.0000 - precision: 0.1016 - recall: 0.9904 - val_loss: 0.2565 - val_fn: 9.0000 - val_fp: 1411.0000 - val_tn: 55475.0000 - val_tp: 66.0000 - val_precision: 0.0447 - val_recall: 0.8800\n",
            "Epoch 45/50\n",
            "227846/227846 - 1s - loss: 7.0807e-07 - fn: 3.0000 - fp: 3798.0000 - tn: 223631.0000 - tp: 414.0000 - precision: 0.0983 - recall: 0.9928 - val_loss: 0.0411 - val_fn: 10.0000 - val_fp: 310.0000 - val_tn: 56576.0000 - val_tp: 65.0000 - val_precision: 0.1733 - val_recall: 0.8667\n",
            "Epoch 46/50\n",
            "227846/227846 - 1s - loss: 2.3221e-07 - fn: 3.0000 - fp: 1965.0000 - tn: 225464.0000 - tp: 414.0000 - precision: 0.1740 - recall: 0.9928 - val_loss: 0.0214 - val_fn: 10.0000 - val_fp: 210.0000 - val_tn: 56676.0000 - val_tp: 65.0000 - val_precision: 0.2364 - val_recall: 0.8667\n",
            "Epoch 47/50\n",
            "227846/227846 - 1s - loss: 1.6100e-07 - fn: 1.0000 - fp: 1461.0000 - tn: 225968.0000 - tp: 416.0000 - precision: 0.2216 - recall: 0.9976 - val_loss: 0.0217 - val_fn: 9.0000 - val_fp: 218.0000 - val_tn: 56668.0000 - val_tp: 66.0000 - val_precision: 0.2324 - val_recall: 0.8800\n",
            "Epoch 48/50\n",
            "227846/227846 - 1s - loss: 1.9255e-07 - fn: 3.0000 - fp: 1707.0000 - tn: 225722.0000 - tp: 414.0000 - precision: 0.1952 - recall: 0.9928 - val_loss: 0.0126 - val_fn: 13.0000 - val_fp: 140.0000 - val_tn: 56746.0000 - val_tp: 62.0000 - val_precision: 0.3069 - val_recall: 0.8267\n",
            "Epoch 49/50\n",
            "227846/227846 - 1s - loss: 3.1400e-07 - fn: 1.0000 - fp: 1505.0000 - tn: 225924.0000 - tp: 416.0000 - precision: 0.2166 - recall: 0.9976 - val_loss: 0.2525 - val_fn: 10.0000 - val_fp: 1164.0000 - val_tn: 55722.0000 - val_tp: 65.0000 - val_precision: 0.0529 - val_recall: 0.8667\n",
            "Epoch 50/50\n",
            "227846/227846 - 1s - loss: 8.4340e-07 - fn: 2.0000 - fp: 4023.0000 - tn: 223406.0000 - tp: 415.0000 - precision: 0.0935 - recall: 0.9952 - val_loss: 0.0765 - val_fn: 9.0000 - val_fp: 441.0000 - val_tn: 56445.0000 - val_tp: 66.0000 - val_precision: 0.1302 - val_recall: 0.8800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1355cde10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdzsLQQgJMib",
        "colab_type": "text"
      },
      "source": [
        "At the end of training, out of 56,961 validation transactions, we are:\n",
        "\n",
        "- Correctly identifying 66 of them as fraudulent\n",
        "- Missing 9 fraudulent transactions\n",
        "- At the cost of incorrectly flagging 441 legitimate transactions\n",
        "\n",
        "In the real world, one would put an even higher weight on class 1,\n",
        "so as to reflect that False Negatives are more costly than False Positives.\n",
        "\n",
        "Next time your credit card gets  declined in an online purchase -- this is why."
      ]
    }
  ]
}